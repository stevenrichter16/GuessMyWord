# Animal 20 Questions game review

## What works well
- **Clear flow and UI scaffolding.** The SwiftUI view wires a complete game loop with question display, answer buttons, optional hint entry, guess card, and a restart path, giving players immediate feedback and control over pacing. The automatic start-on-load task plus the fallback notice keep the experience coherent even when the LLM backend is swapped.【F:20 Questions/ContentView.swift†L3-L220】
- **LLM prompts are tightly controlled.** `PromptBuilder` constrains the model to short yes/no/maybe questions and a single JSON object, while also threading turn counts, hints, and a recent transcript window into both asking and guessing prompts. This reduces rambling model output and keeps prompts grounded in the allowed canon of animals.【F:20 Questions/LLMScaffolding.swift†L70-L101】
- **Deterministic offline gameplay path.** The `AnimalQuestionEngine` and Core ML wrapper allow the game to run without a live LLM by selecting splitting questions from the 44-attribute dataset and producing a model-driven guess. Rotating canned questions/guesses backstop the UI when even the dataset is missing, so the interface never stalls.【F:20 Questions/LLMScaffolding.swift†L123-L229】【F:20 Questions/LLMScaffolding.swift†L480-L558】
- **Data coverage and documentation.** The included design doc and `animals_20q.csv` describe the 100-animal, 44-attribute matrix in detail, making it straightforward to extend the dataset or retrain the Core ML model.【F:20 Questions/20 Questions Animal Game: Design and Data.md†L1-L116】

## Opportunities for improvement
- **Handle ambiguous answers more explicitly.** `currentCandidates` ignores `maybe`/`notSure` replies while scoring splits only on yes/no counts, so the engine may keep asking attributes it already has fuzzy signals for. Weighting "maybe" as partial evidence (e.g., 0.5 probability) or deprioritising already-asked ambiguous features could sharpen question selection.【F:20 Questions/LLMScaffolding.swift†L520-L548】
- **Guard against low-value fallback questions.** When all informative attributes are exhausted or filtered out, the engine falls back to the first unseen feature, which can be poorly discriminative for the remaining candidates. Tracking information gain or remaining candidate entropy would let the engine prefer the strongest splitter instead of a first-come default.【F:20 Questions/LLMScaffolding.swift†L500-L519】
- **Use answer-based confidence smoothing.** The Core ML guess returns uniform 0.25 confidence when probabilities are zeroed, even if many candidates were eliminated. Calibrating confidence based on surviving candidate count or the number of answered attributes would yield more meaningful confidence readouts and user trust.【F:20 Questions/LLMScaffolding.swift†L544-L558】
- **Surface training/evaluation feedback.** There is a `GameSimulator` for offline accuracy checks, but no UI hook or logging summary to expose results. Exposing a developer toggle to run a batch simulation and show accuracy would help validate model updates without Xcode breakpoints.【F:20 Questions/GameSimulation.swift†L1-L62】
- **Resolve hard-coded model availability path.** The default client forces `FoundationModelClient` even when `SystemLanguageModel` reports unavailable, which can create runtime errors on unsupported devices. Adding a user-facing error state or skipping LLM calls in that scenario would prevent confusing failures.【F:20 Questions/LLMScaffolding.swift†L214-L235】

## Quick next steps
- Add a lightweight notion of answer certainty (yes/no/maybe/not sure) into the question scoring function so the engine reduces re-asking fuzzy attributes and improves split quality.
- Replace the "first unseen" fallback with an information-gain metric that always prefers the question that most evenly divides the remaining candidates.
- Calibrate guess confidence using surviving candidate counts and expose a debug-only simulation screen that runs `GameSimulator` batches to show win rate after model changes.
